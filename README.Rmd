---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
set.seed(7)
```

# ytdlpr

<!-- badges: start -->
[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#experimental)
<!-- badges: end -->

`ytdlpr` wraps some functionalities of `yt-dlp` to facilitate their use in R-based workflows. It is currently focused on retrieving and parsing subtitles.

## Installation

You can install the development version of `ytdlpr` from [GitHub](https://github.com/giocomai/ytdlpr) with:

``` r
remotes::install_github("giocomai/ytdlpr")
```

## Documentation 

Let's say we want to retrieve some information about [this playlist](https://www.youtube.com/playlist?list=PLbyvawxScNbtMcDKmT2dRAfjmSFwOt1Vj) of video files published by the European Space Agency (chosen at random). The reference use case is about retrieving subtitles, but documentation about other use cases will be added. 

We will first need to set a base folder where all the files retrieved with this package will be stored. I like to store such things under the R folder of my home directory, so I may proceed as follows:

```{r set_base_folder}
library("ytdlpr")

yt_set_base_folder(path = fs::path(
  fs::path_home_r(),
  "R",
  "ytdlpr" # you'd probably set something meaningful here, relevant to what you're downloading
))
```

Throughout the current session, this will be the main folder where I'll be storing files. If not set, it will default to storing things inside the current working directory. 

What happens next depends a bit on your starting point, i.e., if you have a set of urls of individual videos or if you want to retrieve subtitles about a whole playlist. 

### Retrieve subtitles starting from a playlist

A first step, that if not explicitly done will be performed implicitly by other playlist-based functions is to retrieve all identifiers from the given playlist.

```{r yt_get_playlist_id}
yt_get_playlist_id(
  playlist = "https://www.youtube.com/playlist?list=PLbyvawxScNbuSi7sJaJbHNyyx3iYJeW3P"
)
```
Notice that the data are stored locally: unless the `update` argument in `yt_get_playlist_id()` is set to TRUE, following calls to this function will just retrieve locally stored identifiers.

If all we really care about is subtitles, we can skip this step and move on to downloading subtitles. By default, this will proceed with downloading English sub_lang subtitles, but you can customise this using the dedicated function.

```{r yt_get_subtitles_playlist, eval = FALSE}
yt_get(
  playlist = "https://www.youtube.com/playlist?list=PLbyvawxScNbuSi7sJaJbHNyyx3iYJeW3P",
  subtitles = TRUE
)
```

This will download all relevant subtitles and return a data frame with some basic metadata about them.

The next step is to actually import these subtitles in a format that is easy to parse in R. You can download and import them in a single go with:

```{r yt_get_subtitles_playlist and yt_read_vtt, message = FALSE}
yt_get(
  playlist = "https://www.youtube.com/playlist?list=PLbyvawxScNbuSi7sJaJbHNyyx3iYJeW3P",
  subtitles = TRUE
) |>
  yt_read_vtt()
```

Or, yf you want to parse only subtitles that are locally available, you can achieve the same with `yt_get_local_subtitles()`: 

```{r yt_get_local_subtitles and yt_read_vtt, message = FALSE}
subtitles_df <- yt_get_local_subtitles(
  playlist = "https://www.youtube.com/playlist?list=PLbyvawxScNbuSi7sJaJbHNyyx3iYJeW3P"
) |>
  yt_read_vtt()
```

So... these are lengthy video clips with hours of recordings by the European Space Agency. Let's see all the times when they mention the word "rover":

```{r yt_filter}
rover_df <- yt_filter(
  pattern = "rover",
  subtitles_df = subtitles_df
)
rover_df
```

The resulting data frame includes a direct link to the relevant video clip at the exact timing. Here just a few examples, with just one mention to `rover` per clip.

```{r rover_link_list, results='asis', echo=FALSE}
selected_rover_df <- rover_df |>
  dplyr::group_by(yt_id) |>
  dplyr::slice_sample(n = 1) |>
  dplyr::ungroup()

for (i in selected_rover_df[["link"]]) {
  cat(paste0("- ", i, "\n"))
}
```

Let's try again, with an example that may be more relevant and perhaps more inspiring for possible use cases among R users: all references to "community" in the playlist of Posit Conf 2023:

```{r positconf2023, message = FALSE}
positconf2023_df <- yt_get(
  playlist = "https://www.youtube.com/playlist?list=PL9HYL-VRX0oRFZslRGHwHuwea7SvAATHp",
  subtitles = TRUE
) |>
  yt_read_vtt()

community_df <- yt_filter(
  pattern = "community",
  subtitles_df = positconf2023_df
)
community_df
```

Here just a random sample of examples:

```{r community_link_list, results='asis', echo=FALSE}
selected_community_df <- community_df |>
  dplyr::group_by(yt_id) |>
  dplyr::slice_sample(n = 1) |>
  dplyr::ungroup() |>
  dplyr::slice_sample(n = 6)

for (i in selected_community_df[["link"]]) {
  cat(paste0("- ", i, "\n"))
}
```

## Retrieve subtitles starting from single video urls/id

At the most basic, if you just want to download a single video rather than a whole playlist, there is not much of a difference, you just pass the `yt_id` argument instead of `playlist`. Notice that `yt_id` accepts both YouTube identifiers (the 11-character string mostly at the end of YouTube links) as well as full URLs to a single video clip. 

```{r get_single_video}
yt_get(
  yt_id = "https://youtu.be/WXPBOfRtXQE",
  subtitles = TRUE,
  video = TRUE,
  description = TRUE,
  info_json = TRUE
)
```

## License and disclaimers. 

This package is released with a MIT license. Much of what it does requires external pacakges (in particular, [yt-dlp](https://github.com/yt-dlp/yt-dlp) and [ffmpeg](https://ffmpeg.org/): see their repositories for further license details and disclaimers.
